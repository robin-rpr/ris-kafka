services:
    app:
        build: .
        # image: robinrpr/ris-kafka:latest
        restart: unless-stopped
        environment:
            KAFKA_FQDN: kafka:29092
            ENSURE_CONTINUITY: true # Deadlock in case of unrecoverable message loss
            ENABLE_PROFILING: true # Enable profiling (outputs to a profiles/ directory)
            BUFFER_SIZE: 10000 # Size of the buffer where messages are buffered before sending
            BUFFER_CRITICAL_SIZE: 100 # Critical section size in the buffer to ensure eventual consistency
            TIME_LAG_LIMIT: 10 # Maximum Time lag limit in minutes. If exceeded, the process will terminate.
            BATCH_CONSUME: 1000 # Number of messages to consume in one round from websocket
            BATCH_SEND: 10000 # Number of messages to send in one round to kafka
            REDIS_MAX_CONNECTIONS: 50 # Maximum number of connections to redis
            REDIS_HOST: redis
            REDIS_PORT: 6379
            REDIS_DB: 0
            RIS_HOST: rrc13 # RRC Host
        depends_on:
            redis:
                condition: service_healthy
            kafka:
                condition: service_healthy
        working_dir: /app
        volumes:
            - .:/app
        entrypoint: >
            python app.py

    redis:
        image: redis:latest
        restart: unless-stopped
        healthcheck:
            test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
            interval: 30s
            timeout: 60s
            retries: 10
        volumes:
            - redis_data:/data
        ports:
            - "6379:6379"
        environment:
            - ALLOW_EMPTY_PASSWORD=yes

    zookeeper:
        image: confluentinc/cp-zookeeper:7.7.1
        restart: unless-stopped
        volumes:
            - zookeeper_data:/var/lib/zookeeper
        environment:
            ZOOKEEPER_CLIENT_PORT: 2181
            ZOOKEEPER_TICK_TIME: 2000

    kafka:
        image: confluentinc/cp-kafka:7.7.1
        restart: unless-stopped
        healthcheck:
            test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "kafka:29092"]
            interval: 30s
            timeout: 60s
            retries: 10
        volumes:
            - kafka_data:/var/lib/kafka/data
        ports:
            - "9092:9092"
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
            KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://example.com:9092
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_NUM_PARTITIONS: 1
            KAFKA_LOG_RETENTION_HOURS: 48
            KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 600000
            KAFKA_LOG_ROLL_MS: 3600000
            KAFKA_LOG_SEGMENT_BYTES: 1073741824 # 1GB
            KAFKA_MESSAGE_MAX_BYTES: 100000000
            KAFKA_LOG_CLEANER_THREADS: 2
            KAFKA_COMPRESSION_TYPE: lz4
            KAFKA_NUM_NETWORK_THREADS: 8
            KAFKA_NUM_IO_THREADS: 8

    kafbat:
        image: ghcr.io/kafbat/kafka-ui:latest
        restart: unless-stopped
        ports:
            - 8080:8080
        depends_on:
            kafka:
                condition: service_healthy
        environment:
            KAFKA_CLUSTERS_0_NAME: ris-kafka
            KAFKA_CLUSTERS_0_BOOTSTRAP_SERVERS: kafka:29092
            KAFKA_CLUSTERS_0_READONLY: true

# Define volumes
volumes:
    zookeeper_data:
    kafka_data:
    redis_data: